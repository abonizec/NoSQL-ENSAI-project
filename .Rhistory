prune <- prune.cubt(clust,data,nleaves=4)
# join leaves
leaves <- join.cubt(prune,data,nclass=4, crit0="entropy")
# plot the tree
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(clust,treedata_t[,1:3])
treeerror_cubt <- error(pred, treedata_t$clust)
treeerror_cubt #57%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(treedata[,1:3])
clust <- cubt(data,critopt="entropy",method = "cubt",minsize = 20)
# plot the tree
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(clust,treedata_t[,1:3])
treeerror_cubt <- error(pred, treedata_t$clust)
treeerror_cubt #57%
pred
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(treedata[,1:3])
clust <- cubt(data,critopt="entropy",method = "cubt",minsize = 20)
# plot the tree
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(clust,treedata_t[,1:3])
treeerror_cubt <- error(pred, treedata_t$clust)
treeerror_cubt #57%
d <- dist(breast[,1:30])
d <- dist(breast_l[,1:30])
breasthc <- hclust(d,method="ward.D")
plot(breasthc,ylab="Distance",main="Dendrogram")
breasthc2 <- cutree(breasthc,k=2)
error(breasthc2, breast_l$Y)
plotcluster(breast[, 1:30], breasthc2)
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(breast_l[,1:30])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
breasthc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(breasthc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 2
breasthc2 <- cutree(breasthc,k=2)
#Misclassification rate
error(breasthc2, breast_l$Y)
#The misclassification rate is 13.42%
#Misclassification rate
berror_hc <- error(breasthc2, breast_l$Y)
berror_hc
data <- as.matrix(breast_l[,1:30])
clust <- cubt(data,critopt="entropy",method = "cubt")
View(data)
clust <- cubt(data,method = "cubt")
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
prune <- prune.cubt(clust,data,nleaves=2)
leaves <- join.cubt(prune,data,nclass=2)
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
pred <- cl_predict(leaves,breast_t[,1:30])
pred <- cl_predict(leaves,breast_t[,1:30])
berror_cubt <- error(pred, lcdata_t$clust)
berror_cubt #20%
berror_cubt <- error(pred, breast_t$Y)
berror_cubt #20%
pred
berror_cubt <- error(pred, breast_t$Y)
berror_cubt #20%
?predict.clus
?predict.cubt
?predict.clus
data <- as.matrix(lcdata[,1:9])
str(data)
?as.matrix
treedata[,1] <- as.character(treedata[,1])
str(treedata)
treedata[,2] <- as.character(treedata[,2])
treedata[,3] <- as.character(treedata[,3])
str(treedata)
treekm <- kmeans(treedata[, 1:3],4)
plotcluster(treedata[, 1:3], treekm$cluster)
pred <- cl_predict(treekm,treedata_t[,1:3])
treeerror_km <- error(pred, treedata_t$clust)
treeerror_km #63%
treedata_t[,1] <- as.character(treedata_t[,1])
treedata_t[,2] <- as.character(treedata_t[,2])
treedata_t[,3] <- as.character(treedata_t[,3])
pred <- cl_predict(treekm,treedata_t[,1:3])
treeerror_km <- error(pred, treedata_t$clust)
treeerror_km #63%
pred <- cl_predict(treekm,treedata_t[,1:3])
set.seed(10)
lcdata <- data.frame(lc(9,100,5,0.8))
treedata <- data.frame(tree(100))
set.seed(20)
lcdata_t <- data.frame(lc(9,100,5,0.8))
treedata_t <- data.frame(tree(100))
#*******************************************************************#
#------ CLUSTERING METHODS : Application in the LC simulation ------#
#*******************************************************************#
### K-means method ###
lckm <- kmeans(lcdata[, 1:9],3)
#Plotting the clusters defined by K-means method
plotcluster(lcdata[, 1:9], lckm$cluster)
#Misclassification rate
pred <- cl_predict(lckm,lcdata_t[,1:9])
lcerror_km <- error(pred, lcdata_t$clust)
lcerror_km
#The misclassification rate is 2%
set.seed(10)
lcdata <- data.frame(lc(9,100,5,0.8))
treedata <- data.frame(tree(100))
set.seed(20)
lcdata_t <- data.frame(lc(9,100,5,0.8))
treedata_t <- data.frame(tree(100))
#*******************************************************************#
#------ CLUSTERING METHODS : Application in the LC simulation ------#
#*******************************************************************#
### K-means method ###
lckm <- kmeans(lcdata[, 1:9],3)
#Plotting the clusters defined by K-means method
plotcluster(lcdata[, 1:9], lckm$cluster)
#Misclassification rate
pred <- cl_predict(lckm,lcdata_t[,1:9])
lcerror_km <- error(pred, lcdata_t$clust)
lcerror_km
#The misclassification rate is 2%
lc = function(p,n,m,q){
#Matrix initialization to save the data
X <- matrix(data=NA,nrow=n,ncol=p)
#Probability setting
pset <- (1-q)/(m-1)
#Size of each group
gp <- round(n/3)
for (p in c(1:p)){
#Custer1
X[1:gp,p]  <- sample(x = c(1:m), gp, replace = T, prob = c(q, rep(pset,(m-1))))
#Cluster2
X[(gp+1):(2*gp),p]  <- sample(x = c(1:m), gp, replace = T, prob = c(rep(pset,2),q,rep(pset,(m-3))))
#Cluster3
X[(((2*gp))+1) :n,p]  <- sample(x = c(1:m), (n-2*gp), replace = T, prob = c(rep(pset,4),q,rep(pset,(m-5))))
}
#Add the name of the cluster
clust <- c(rep(1,gp),rep(2,gp),rep(3,(n-2*gp)))
X <- cbind(X,clust)
return(X=X)
}
set.seed(10)
lcdata <- data.frame(lc(9,100,5,0.8))
set.seed(20)
lcdata_t <- data.frame(lc(9,100,5,0.8))
View(lcdata)
lckm <- kmeans(lcdata[, 1:9],3)
plotcluster(lcdata[, 1:9], lckm$cluster)
pred <- cl_predict(lckm,lcdata_t[,1:9])
lcerror_km <- error(pred, lcdata_t$clust)
lcerror_km
#*******************************************************************#
#------ CLUSTERING METHODS : Application in the LC simulation ------#
#*******************************************************************#
### K-means method ###
lckm <- kmeans(lcdata[, 1:9],3)
#Plotting the clusters defined by K-means method
plotcluster(lcdata[, 1:9], lckm$cluster)
#Misclassification rate
pred <- cl_predict(lckm,lcdata_t[,1:9])
lcerror_km <- error(pred, lcdata_t$clust)
lcerror_km
#The misclassification rate is 2%
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(lcdata[,1:9])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
lchc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(lchc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 3
lchc3 <- cutree(lchc,k=3)
#Misclassification rate
lc_error_hc <- error(lchc3, lcdata$clust)
lc_error_hc
#The misclassification rate is 1%
?cubt
data <- as.matrix(lcdata[,1:9])
clust <- cubt(data,critopt="entropy",method = "cubt",minsize=opt.minsize())
clust <- cubt(data,critopt="entropy",method = "cubt",minsize=30)
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
clust <- cubt(data,critopt="entropy",method = "cubt")
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
prune <- prune.cubt(clust,data,nleaves=3)
leaves <- join.cubt(prune,data,nclass=3, crit0="entropy")
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
pred <- cl_predict(leaves,lcdata_t[,1:9])
lcerror_cubt <- error(pred, lcdata_t$clust)
lcerror_cubt #20%
#*********************************************************************#
#------ CLUSTERING METHODS : Application in the Tree simulation ------#
#*********************************************************************#
### K-means method ###
treekm <- kmeans(treedata[, 1:3],4)
#Misclassification rate
pred <- cl_predict(treekm,treedata_t[,1:3])
treeerror_km <- error(pred, treedata_t$clust)
treeerror_km #63%
#Plotting the clusters defined by K-means method
plotcluster(treedata[, 1:3], treekm$cluster)
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(treedata[,1:3])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
treehc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(treehc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 4
treehc4 <- cutree(treehc,k=4)
#Misclassification rate
treeerror_hc <- error(treehc4, treedata$clust)
treeerror_hc
#The misclassification rate is 63%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(treedata[,1:3])
clust <- cubt(data,critopt="entropy",method = "cubt",minsize = 20)
# plot the tree
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(clust,treedata_t[,1:3])
treeerror_cubt <- error(pred, treedata_t$clust)
treeerror_cubt #57%
#*********************************************************************#
#------ CLUSTERING METHODS : Real data from supervise learning  ------#
#*********************************************************************#
### K-means method ###
#We want two clusters (B/M)
breastkm <- kmeans(breast_l[, 1:30],2)
#Misclassification rate
pred <- cl_predict(breastkm,breast_t[,1:30])
berror_km <- error(pred, breast_t$Y)
berror_km
#The misclassification rate is 15.34%
#Plotting the clusters defined by K-means method
plotcluster(breast_l[, 1:30], breastkm$cluster)
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(breast_l[,1:30])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
breasthc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(breasthc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 2
breasthc2 <- cutree(breasthc,k=2)
#Misclassification rate
berror_hc <- error(breasthc2, breast_l$Y)
berror_hc
#The misclassification rate is 13.42%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(breast_l[,1:30])
clust <- cubt(data,method = "cubt")
# prune it
prune <- prune.cubt(clust,data,nleaves=2)
# join leaves
leaves <- join.cubt(prune,data,nclass=2)
# plot the tree
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(leaves,breast_t[,1:30])
berror_cubt <- error(pred, breast_t$Y)
berror_cubt #19.58%
breastkm <- kmeans(breast_l[, 1:30],2)
breastkm
names(breastkm)
?kmeans
#Loads the libraries
library("clue")
library("partitions")
library("cubt")
library(cluster)
library(fpc)
#********************************#
#------ DATASET SIMULATION ------#
#********************************#
### LINEAR COMBINATION
# A function will simulate the data according to 4 parameters:
# p: Number of discrete variables
# n: Total number of observations
# m: Number of modalities for the discrete variables
# q: Probability to obtain the level chosen to be the most frequent
# The number of clusters is 3.
lc = function(p,n,m,q){
#Matrix initialization to save the data
X <- matrix(data=NA,nrow=n,ncol=p)
#Probability setting
pset <- (1-q)/(m-1)
#Size of each group
gp <- round(n/3)
for (p in c(1:p)){
#Custer1
X[1:gp,p]  <- sample(x = c(1:m), gp, replace = T, prob = c(q, rep(pset,(m-1))))
#Cluster2
X[(gp+1):(2*gp),p]  <- sample(x = c(1:m), gp, replace = T, prob = c(rep(pset,2),q,rep(pset,(m-3))))
#Cluster3
X[(((2*gp))+1) :n,p]  <- sample(x = c(1:m), (n-2*gp), replace = T, prob = c(rep(pset,4),q,rep(pset,(m-5))))
}
#Add the name of the cluster
clust <- c(rep(1,gp),rep(2,gp),rep(3,(n-2*gp)))
X <- cbind(X,clust)
return(X=X)
}
### TREE MODEL
# A function will simulate the data according to the size of the sample (parameter n)
# There will be 4 clusters, 3 discretes variables with 6 modalities, and a probality fixed at 0.8/0.2 for the generation of odds.
tree = function(n){
#Matrix initialization to save the data
X <- matrix(data=NA,nrow=n,ncol=3)
#Size of each group
gp <- round(n/4)
#Probabilities setting
p1 <- 1
p2 <- 0
#Custer1
X[1:gp,1]  <- sample(x = c(1:6), gp, replace = T, prob = c(p1,p2,p1,p2,p1,p2))
X[1:gp,2]  <- sample(x = c(1:6), gp, replace = T, prob = c(p1,p2,p1,p2,p1,p2))
X[1:gp,3]  <- sample(x = c(1:6), gp, replace = T)
#Cluster2
X[(gp+1):(2*gp),1]  <- sample(x = c(1:6), gp, replace = T, prob = c(p1,p2,p1,p2,p1,p2))
X[(gp+1):(2*gp),2]  <- sample(x = c(1:6), gp, replace = T, prob = c(p2,p1,p2,p1,p2,p1))
X[(gp+1):(2*gp),3]  <- sample(x = c(1:6), gp, replace = T)
#Cluster3
X[(2*gp+1):(3*gp),1]  <- sample(x = c(1:6), gp, replace = T, prob = c(p2,p1,p2,p1,p2,p1))
X[(2*gp+1):(3*gp),2]  <- sample(x = c(1:6), gp, replace = T)
X[(2*gp+1):(3*gp),3]  <- sample(x = c(1:6), gp, replace = T, prob = c(p1,p2,p1,p2,p1,p2))
#Cluster4
X[(3*gp+1):n,1]  <- sample(x = c(1:6), (n-3*gp), replace = T, prob = c(p2,p1,p2,p1,p2,p1))
X[(3*gp+1):n,2]  <- sample(x = c(1:6), (n-3*gp), replace = T)
X[(3*gp+1):n,3]  <- sample(x = c(1:6), (n-3*gp), replace = T, prob = c(p2,p1,p2,p1,p2,p1))
#Add the cluster number
clust <- c(rep(1,gp),rep(2,gp),rep(3,gp),rep(4,(n-3*gp)))
X <- cbind(X,clust)
X[,1] <- as.integer(X[,1] )
X[,2] <- as.integer(X[,2] )
X[,3] <- as.integer(X[,3] )
return(X=X)
}
### REAL DATA FROM SUPERVISED LEARNING
#Set working directory
setwd("~/workspace/Complements on Unsupervised Learning")
#Importation of the database (No Missing Data as mentioned in the documentation)
breast <- read.table(file= "wdbc.data", header=FALSE,sep=",",col.names = c("ID","diagnosis",paste("X",1:30,sep="")))
#Copy the diagnosis into a variable Y at the end of the dataset
breast$Y <- breast$diagnosis
#Removing the variable ID which is useless for the analysis and the variable diagnosis (Y will be used)
breast <- breast[,-c(1,2)]
### LEARNING AND TEST SAMPLES
set.seed(10)
lcdata <- data.frame(lc(9,100,5,0.8))
treedata <- data.frame(tree(100))
set.seed(20)
lcdata_t <- data.frame(lc(9,100,5,0.8))
treedata_t <- data.frame(tree(100))
split1 <- sample(1:nrow(breast),nrow(breast)/3)
split2 <- (-split1)
breast_t <- breast[split1,]
breast_l <- breast[split2,]
#*******************************************************************#
#------ CLUSTERING METHODS : Application in the LC simulation ------#
#*******************************************************************#
### K-means method ###
lckm <- kmeans(lcdata[, 1:9],3)
#Plotting the clusters defined by K-means method
plotcluster(lcdata[, 1:9], lckm$cluster)
#Misclassification rate
pred <- cl_predict(lckm,lcdata_t[,1:9])
lcerror_km <- error(pred, lcdata_t$clust)
lcerror_km
#The misclassification rate is 2%
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(lcdata[,1:9])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
lchc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(lchc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 3
lchc3 <- cutree(lchc,k=3)
#Misclassification rate
lc_error_hc <- error(lchc3, lcdata$clust)
lc_error_hc
#The misclassification rate is 1%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(lcdata[,1:9])
clust <- cubt(data,critopt="entropy",method = "cubt")
?cubt
# prune it
prune <- prune.cubt(clust,data,nleaves=3)
# join leaves
leaves <- join.cubt(prune,data,nclass=3, crit0="entropy")
# plot the tree
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(leaves,lcdata_t[,1:9])
lcerror_cubt <- error(pred, lcdata_t$clust)
lcerror_cubt #20%
#*********************************************************************#
#------ CLUSTERING METHODS : Application in the Tree simulation ------#
#*********************************************************************#
### K-means method ###
treekm <- kmeans(treedata[, 1:3],4)
#Misclassification rate
pred <- cl_predict(treekm,treedata_t[,1:3])
treeerror_km <- error(pred, treedata_t$clust)
treeerror_km #60%
#Plotting the clusters defined by K-means method
plotcluster(treedata[, 1:3], treekm$cluster)
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(treedata[,1:3])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
treehc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(treehc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 4
treehc4 <- cutree(treehc,k=4)
#Misclassification rate
treeerror_hc <- error(treehc4, treedata$clust)
treeerror_hc
#The misclassification rate is 63%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(treedata[,1:3])
clust <- cubt(data,critopt="entropy",method = "cubt",minsize = 20)
# plot the tree
par(xpd = TRUE)
plot(clust,type="u")
text(clust, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(clust,treedata_t[,1:3])
treeerror_cubt <- error(pred, treedata_t$clust)
treeerror_cubt #57%
#*********************************************************************#
#------ CLUSTERING METHODS : Real data from supervise learning  ------#
#*********************************************************************#
### K-means method ###
#We want two clusters (B/M)
breastkm <- kmeans(breast_l[, 1:30],2)
#Misclassification rate
pred <- cl_predict(breastkm,breast_t[,1:30])
berror_km <- error(pred, breast_t$Y)
berror_km
#The misclassification rate is 15.34%
#Plotting the clusters defined by K-means method
plotcluster(breast_l[, 1:30], breastkm$cluster)
### Hierarchical Clustering ###
#Computing a matix of distances
d <- dist(breast_l[,1:30])
#Obtaining a hierarchical clustering
#The agglomeration method used will be the ward's method.
breasthc <- hclust(d,method="ward.D")
# Representing the dendrogram
plot(breasthc,ylab="Distance",main="Dendrogram")
#Obtaining a clustering with a level 2
breasthc2 <- cutree(breasthc,k=2)
#Misclassification rate
berror_hc <- error(breasthc2, breast_l$Y)
berror_hc
#The misclassification rate is 13.42%
### CUBT - Clustering unsupervised binary tree ###
# construct the maximal tree
data <- as.matrix(breast_l[,1:30])
clust <- cubt(data,method = "cubt")
# prune it
prune <- prune.cubt(clust,data,nleaves=2)
# join leaves
leaves <- join.cubt(prune,data,nclass=2)
# plot the tree
par(xpd = TRUE)
plot(leaves,type="u")
text(leaves, all = TRUE)
#Predict from the learning sample, the cluster of each observation from the test sample
pred <- cl_predict(leaves,breast_t[,1:30])
berror_cubt <- error(pred, breast_t$Y)
berror_cubt #19.58%
